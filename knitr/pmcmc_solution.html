<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Solution: the particle MCMC</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="github-pandoc.css" type="text/css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Solution: the particle MCMC</h1>
</div>
<div id="TOC">
<ul>
<li><a href="#run-a-particle-filter">Run a particle filter</a></li>
<li><a href="#calibrate-the-number-of-particles">Calibrate the number of particles</a></li>
<li><a href="#setting-the-pmcmc">Setting the pMCMC</a></li>
<li><a href="#analyse-a-pmcmc-with-50-particles">Analyse a pMCMC with 50 particles</a></li>
<li><a href="#analyse-a-pmcmc-with-400-particles">Analyse a pMCMC with 400 particles</a></li>
<li><a href="#stochastic-vs-deterministic-fit">Stochastic vs deterministic fit</a></li>
</ul>
</div>
<h1 id="run-a-particle-filter">Run a particle filter</h1>
<p>The particle filter returns a Monte-Carlo estimate of the log-likelihood and, as every Monte-carlo estimate, its precision depends on the number of particles.</p>
<p>If you have too few particles then you will have a highly variable estimate of the log-likelihood and this will make the exploration of the likelihood surface quite imprecise. In addition, you might experience particle depletion (if you don't know what that means just try to run a particle filter with a single particle).</p>
<p>If you have too many particles, then you will have an accurate estimate of your log-likelihood but it will be very time consuming so inefficient in practice.</p>
<p>In addition, the variability of the estimate might also depend on the region of the parameter space. For instance, in some region you might experience significant variability with 100 particles whereas in another region it might be fairly stable.</p>
<p>You can now return to the <a href="pmcmc.html#calibrate-the-number-of-particles">practical</a> and try to think on a way to calibrate the number of particles.</p>
<h1 id="calibrate-the-number-of-particles">Calibrate the number of particles</h1>
<p>Ideally we want enough particles to obtain a fairly stable estimate of the log-likelihood in a reasonable computational time.</p>
<p>A simple calibration approach consists in plotting the mean, standard deviation and computational time of the log-likelihood estimate as a function of the number of particles. Although several parameters can be tested, we will use a <code>theta</code> close to the mean posterior estimate of the deterministic fit since it is likely to be the region of the parameter space we want to explore with the pMCMC.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load fitmodel, data and define init.state</span>
<span class="kw">example</span>(SEIT2L_stochastic)
<span class="kw">data</span>(FluTdC1971)
init.state &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">S =</span> <span class="dv">279</span>, <span class="dt">E =</span> <span class="dv">0</span>, <span class="dt">I =</span> <span class="dv">2</span>, <span class="dt">T1 =</span> <span class="dv">3</span>, <span class="dt">T2 =</span> <span class="dv">0</span>, <span class="dt">L =</span> <span class="dv">0</span>, <span class="dt">Inc =</span> <span class="dv">0</span>)

<span class="co"># pick a theta close to the mean posterior estimate of the deterministic fit</span>
theta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">R0 =</span> <span class="dv">7</span>, <span class="dt">D.lat =</span> <span class="dv">1</span>, <span class="dt">D.inf =</span> <span class="dv">4</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">D.imm =</span> <span class="dv">10</span>, <span class="dt">rho =</span> <span class="fl">0.65</span>)

<span class="co"># vector of number of particles to test</span>
test.n.particles &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">50</span>, <span class="dv">1000</span>, <span class="dv">50</span>)

<span class="co"># number of replicates</span>
n.replicates &lt;-<span class="st"> </span><span class="dv">100</span>

<span class="co"># vector and data frame of results</span>
sample.log.like &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dt">length =</span> n.replicates)
res &lt;-<span class="st"> </span><span class="kw">data.frame</span>()

for (n.particles in test.n.particles) {
    
    <span class="co"># start measuring time</span>
    start.time &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
    for (i in <span class="dv">1</span>:n.replicates) {
        <span class="co"># one Monte-Carlo estimate of the log-likelihood</span>
        sample.log.like[i] &lt;-<span class="st"> </span><span class="kw">my_particleFilter</span>(SEIT2L_sto, theta, init.state, 
            FluTdC1971, n.particles)
    }
    <span class="co"># end measuring time</span>
    end.time &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
    
    <span class="co"># keep only replicate with finite log-likelihood to be able to compute the</span>
    <span class="co"># mean and sd this give us the proportion of replicates with particle</span>
    <span class="co"># depletion.</span>
    sample.finite.log.like &lt;-<span class="st"> </span>sample.log.like[<span class="kw">is.finite</span>(sample.log.like)]
    
    ans &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(sample.finite.log.like), <span class="dt">sd =</span> <span class="kw">sd</span>(sample.finite.log.like), 
        <span class="dt">prop.depletion =</span> <span class="dv">1</span> -<span class="st"> </span><span class="kw">length</span>(sample.finite.log.like)/<span class="kw">length</span>(sample.log.like), 
        <span class="dt">time =</span> end.time -<span class="st"> </span>start.time)
    
    res &lt;-<span class="st"> </span><span class="kw">rbind</span>(res, <span class="kw">t</span>(ans))
}</code></pre>
<p>We ran this calibration algorithm and obtained the following results:</p>
<p><img src="figure/pmcmc/plot-calibration.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>The mean and standard error of the log-likelihood estimate seem to stabilize around 400 particles (red line). By contrast, the computational time increases linearly with the number of particles. Using 400 particles looks optimal if we want a stable estimator while minimizing the computational time. Note however that it will take around 6 days to run 10000 times the particle filter with 400 particles. Since every step of the pMCMC requires to run a particle filter, that means that it will take 6 days to run 10000 iterations of the pMCMC. This is why you generally need a scientific computing cluster to run a pMCMC.</p>
<p>You can now return to the <a href="pmcmc.html#setting-the-pmcmc">practical</a> and set your pMCMC with 400 particles.</p>
<h1 id="setting-the-pmcmc">Setting the pMCMC</h1>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the fitmodel</span>
<span class="kw">example</span>(SEIT2L_stochastic)

<span class="co"># wrapper for posterior</span>
my_posteriorSto &lt;-<span class="st"> </span>function(theta) {
    
    my_fitmodel &lt;-<span class="st"> </span>SEIT2L_sto
    my_init.state &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">S =</span> <span class="dv">279</span>, <span class="dt">E =</span> <span class="dv">0</span>, <span class="dt">I =</span> <span class="dv">2</span>, <span class="dt">T1 =</span> <span class="dv">3</span>, <span class="dt">T2 =</span> <span class="dv">0</span>, <span class="dt">L =</span> <span class="dv">0</span>, <span class="dt">Inc =</span> <span class="dv">0</span>)
    my_n.particles &lt;-<span class="st"> </span><span class="dv">400</span>
    <span class="co"># you can reduce the number of particles if your pMCMC is too slow</span>
    
    <span class="kw">return</span>(<span class="kw">logPosterior</span>(<span class="dt">fitmodel =</span> my_fitmodel, <span class="dt">theta =</span> theta, <span class="dt">init.state =</span> my_init.state, 
        <span class="dt">data =</span> FluTdC1971, <span class="dt">margLogLike =</span> my_particleFilter, <span class="dt">n.particles =</span> my_n.particles))
    
}

<span class="co"># load results of deterministic fit</span>
<span class="kw">data</span>(mcmc_TdC_deter_longRun)

<span class="co"># Let&#39;s use the first trace only, no need to burn or thin</span>
trace &lt;-<span class="st"> </span>mcmc_SEITL_infoPrior_theta1$trace

<span class="co"># we will start the pMCMC at the mean posterior estimate of the</span>
<span class="co"># deterministic fit</span>
init.theta &lt;-<span class="st"> </span><span class="kw">colMeans</span>(trace[SEIT2L_sto$theta.names])

<span class="co"># and we take the empirical covariance matrix for the Gaussian kernel</span>
<span class="co"># proposal</span>
covmat &lt;-<span class="st"> </span>mcmc_SEITL_infoPrior_theta1$covmat.empirical

<span class="co"># lower and upper limits of each parameter</span>
lower &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">R0 =</span> <span class="dv">0</span>, <span class="dt">D.lat =</span> <span class="dv">0</span>, <span class="dt">D.inf =</span> <span class="dv">0</span>, <span class="dt">alpha =</span> <span class="dv">0</span>, <span class="dt">D.imm =</span> <span class="dv">0</span>, <span class="dt">rho =</span> <span class="dv">0</span>)
upper &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">R0 =</span> <span class="ot">Inf</span>, <span class="dt">D.lat =</span> <span class="ot">Inf</span>, <span class="dt">D.inf =</span> <span class="ot">Inf</span>, <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">D.imm =</span> <span class="ot">Inf</span>, <span class="dt">rho =</span> <span class="dv">1</span>)

<span class="co"># number of iterations for the MCMC</span>
n.iterations &lt;-<span class="st"> </span><span class="dv">50</span>  <span class="co"># just a few since it takes quite a lot of time</span>

<span class="co"># Here we don&#39;t adapt so that we can check the acceptance rate of the</span>
<span class="co"># empirical covariance matrix</span>
adapt.size.start &lt;-<span class="st"> </span><span class="dv">100</span>
adapt.size.cooling &lt;-<span class="st"> </span><span class="fl">0.99</span>
adapt.shape.start &lt;-<span class="st"> </span><span class="dv">100</span></code></pre>
<p>You can now go back to the <a href="pmcmc.html#run-a-pmcmc">practical</a> and try to run a pMCMC with those settings.</p>
<h1 id="analyse-a-pmcmc-with-50-particles">Analyse a pMCMC with 50 particles</h1>
<p>Here is an example of analysis of our 5 chains of 3000 iterations with 50 particles.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load traces</span>
<span class="kw">data</span>(pmcmc_SEIT2L_infoPrior_n50)

<span class="co"># combine into a `mcmc.list` object</span>
trace &lt;-<span class="st"> </span><span class="kw">mcmc.list</span>(<span class="kw">lapply</span>(pmcmc_SEIT2L_infoPrior_n50, function(chain) {
    <span class="kw">mcmc</span>(chain$trace)
}))

<span class="co"># acceptance rate is quite low</span>
<span class="dv">1</span> -<span class="st"> </span><span class="kw">rejectionRate</span>(trace)
##             R0          D.lat          D.inf          alpha          D.imm 
##         0.1277         0.1277         0.1277         0.1277         0.1277 
##            rho      log.prior log.likelihood  log.posterior 
##         0.1277         0.1277         0.1277         0.1277

<span class="co"># accordingly, the combined ESS is quite low</span>
<span class="kw">effectiveSize</span>(trace)
##             R0          D.lat          D.inf          alpha          D.imm 
##          124.1          147.5          213.1          164.0          147.8 
##            rho      log.prior log.likelihood  log.posterior 
##          181.5          272.0          232.9          222.5

<span class="co"># Let&#39;s have a look at the traces</span>
<span class="kw">xyplot</span>(trace)</code></pre>
<p><img src="figure/pmcmc/analyse-n50.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>The burning period looks relatively short, this is because we started the chains with a &quot;good&quot; <code>init.theta</code>. That said, since we have relatively short runs it might be useful to carefully choose the burning period with <code>plotESSBurn</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># this can take some time as we have 5 chains</span>
<span class="kw">plotESSBurn</span>(trace)</code></pre>
<p><img src="figure/pmcmc/analyse-n50-burn1.png" title="" alt="" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># let&#39;s burn the 500 first iterations</span>
trace.burn &lt;-<span class="st"> </span><span class="kw">burnAndThin</span>(trace, <span class="dt">burn =</span> <span class="dv">500</span>)

<span class="co"># what about autocorrelation</span>
<span class="kw">acfplot</span>(<span class="dt">x =</span> trace.burn, <span class="dt">lag.max =</span> <span class="dv">50</span>)</code></pre>
<p><img src="figure/pmcmc/analyse-n50-burn2.png" title="" alt="" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># Although there is significant autocorrelation, we can&#39;t thin too much</span>
<span class="co"># since the chains are quite short.  So let&#39;s keep 1 iteration every 20</span>
trace.burn.thin.n50 &lt;-<span class="st"> </span><span class="kw">burnAndThin</span>(trace.burn, <span class="dt">thin =</span> <span class="dv">20</span>)

<span class="co"># Finally we can plot the posterior density</span>
<span class="kw">densityplot</span>(<span class="dt">x =</span> trace.burn.thin.n50)</code></pre>
<p><img src="figure/pmcmc/analyse-n50-burn3.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>The 5 chains converged to somewhat different posterior distributions, with different shapes and modes. This is particularly evident for the posterior of <span class="math">\(R_0\)</span>, which looks bimodal. In addition, the ESS is too small to have smooth posterior distributions.</p>
<p>With so few particles, the likelihood estimate is very noisy and the exploration of the parameter space is not efficient. This is why we have a lot of autocorrelation and small ESSs. That said, the theoretical properties of the pMCMC guaranty that the chain will converge to the true posterior, even with 1 particle. Of course, this will take a lot of iterations so in practice it might be more efficient to spend more time computing the likelihood (i.e. having more particles) in order to the number of iterations.</p>
<p>You can now return to the <a href="pmcmc.html#analyse-a-pmcmc-with-400-particles">practical</a> and analyse a pMCMC with much more particles that we ran for you.</p>
<h1 id="analyse-a-pmcmc-with-400-particles">Analyse a pMCMC with 400 particles</h1>
<p>Here is an example of analysis of our 5 chains of 3000 iterations with 400 particles.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load traces</span>
<span class="kw">data</span>(pmcmc_SEIT2L_infoPrior_n400)

<span class="co"># combine into a `mcmc.list` object</span>
trace &lt;-<span class="st"> </span><span class="kw">mcmc.list</span>(<span class="kw">lapply</span>(pmcmc_SEIT2L_infoPrior_n400, function(chain) {
    <span class="kw">mcmc</span>(chain$trace)
}))

<span class="co"># acceptance rate is optimal</span>
<span class="dv">1</span> -<span class="st"> </span><span class="kw">rejectionRate</span>(trace)
##             R0          D.lat          D.inf          alpha          D.imm 
##         0.2375         0.2375         0.2375         0.2375         0.2375 
##            rho      log.prior log.likelihood  log.posterior 
##         0.2375         0.2375         0.2375         0.2375

<span class="co"># Note that the combined ESS is 5 times higher than with 50 particles</span>
<span class="kw">effectiveSize</span>(trace)
##             R0          D.lat          D.inf          alpha          D.imm 
##          470.7          538.1          714.1          562.7          470.4 
##            rho      log.prior log.likelihood  log.posterior 
##          599.9          965.0          569.8          571.1

<span class="co"># Let&#39;s have a look at the traces</span>
<span class="kw">xyplot</span>(trace)</code></pre>
<p><img src="figure/pmcmc/analyse-n400.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>As in the analysis with 50 particles, the burning period is relatively short. However, with 400 particles the chains mix much better. Of course the chains take longer to run but because they mix better it becomes computationally more efficient to run many short chains in parallel, starting near the mode of the (deterministic) posterior, and combine them to increase the overall ESS.</p>
<p>Here again we decide to carefully choose the burning period with <code>plotESSBurn</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Actually, it looks like no burning is needed:</span>
<span class="kw">plotESSBurn</span>(trace)</code></pre>
<p><img src="figure/pmcmc/analyse-n400-burn1.png" title="" alt="" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># What about autocorrelation?</span>
<span class="kw">acfplot</span>(<span class="dt">x =</span> trace, <span class="dt">lag.max =</span> <span class="dv">50</span>)</code></pre>
<p><img src="figure/pmcmc/analyse-n400-burn2.png" title="" alt="" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># Autocorrelation decreases much more quickly than with 50 particles Let&#39;s</span>
<span class="co"># keep 1 iteration every 20</span>
trace.thin.n400 &lt;-<span class="st"> </span><span class="kw">burnAndThin</span>(trace, <span class="dt">thin =</span> <span class="dv">20</span>)

<span class="co"># Let&#39;s plot the posterior densities</span>
<span class="kw">densityplot</span>(<span class="dt">x =</span> trace.thin.n400)</code></pre>
<p><img src="figure/pmcmc/analyse-n400-burn3.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>All 5 chains seems to have converged to the same posterior, which are smoother than with 50 particles. Let's compare the combined posterior densities with that obtained with 50 particles</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotPosteriorDensity</span>(<span class="kw">list</span>(<span class="dt">n50 =</span> trace.burn.thin.n50, <span class="dt">n400 =</span> trace.thin.n400))</code></pre>
<p><img src="figure/pmcmc/n400-n50.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>Although the posterior distributions are similar, those with 400 particles are smoother and more representative thanks to higher ESS. In addition, we can notice substantial differences in the location of the mode of <span class="math">\(R_0\)</span>, which is shifted to the left with 50 particles. Finally, note that the log-likelihood is overestimated with 50 particles, which can be problematic when doing model selection as we would overestimate the fit of the model.</p>
<p>Overall, this confirms that the pMCMC works even with 50 particles but that it will require much more iterations to achieve the same posterior as the pMCMC with 400 particles. Although the latter takes more time at each iteration, it provides more reliable estimates on short-runs.</p>
<p>You can now return to the <a href="pmcmc.html#stochastic-vs-deterministic-fit">practical</a> and proceed to the last section of this session.</p>
<h1 id="stochastic-vs-deterministic-fit">Stochastic vs deterministic fit</h1>
<p>Here we compare the combined traces of the deterministic SEIT2L model (2 chains of <span class="math">\(10^5\)</span> iterations) with those obtained with the stochastic version (5 chains of <span class="math">\(3000\)</span> iterations). Both analysis assume informative priors.</p>
<pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># load, burn and thin the deterministic fit</span>

<span class="co"># create mcmc object</span>
trace1 &lt;-<span class="st"> </span><span class="kw">mcmc</span>(mcmc_SEIT2L_infoPrior_theta1$trace)
trace2 &lt;-<span class="st"> </span><span class="kw">mcmc</span>(mcmc_SEIT2L_infoPrior_theta2$trace)

<span class="co"># combine in a mcmc.list</span>
trace &lt;-<span class="st"> </span><span class="kw">mcmc.list</span>(trace1, trace2)

<span class="co"># burn and thin as the chain with uniform prior (see above sections)</span>
trace.deter &lt;-<span class="st"> </span><span class="kw">burnAndThin</span>(trace, <span class="dt">burn =</span> <span class="dv">5000</span>, <span class="dt">thin =</span> <span class="dv">40</span>)

<span class="co"># compare posterior density</span>
<span class="kw">plotPosteriorDensity</span>(<span class="kw">list</span>(<span class="dt">deter =</span> trace.deter, <span class="dt">sto =</span> trace.thin.n400))</code></pre>
<p><img src="figure/pmcmc/sto-deter.png" title="" alt="" style="display: block; margin: auto;" /></p>
<p>Overall, the posterior distributions are quite different. This is especially true for <span class="math">\(R_0\)</span> and <span class="math">\(D_{imm}\)</span>. In addition, the discrepancy in the posterior distribution of the log-likelihood seems to indicate that the stochastic model fits much better. We can verify this by computing the DIC of the stochastic SEIT2L model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># combine all traces in a data frame</span>
trace.combined &lt;-<span class="st"> </span><span class="kw">ldply</span>(trace.thin.n400)

<span class="co"># take the mean of theta</span>
theta.bar &lt;-<span class="st"> </span><span class="kw">colMeans</span>(trace.combined[SEIT2L_sto$theta.names])
## Error: object &#39;SEIT2L_sto&#39; not found
<span class="kw">print</span>(theta.bar)
## Error: object &#39;theta.bar&#39; not found

<span class="co"># compute its log-likelihood</span>
init.state &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">S =</span> <span class="dv">279</span>, <span class="dt">E =</span> <span class="dv">0</span>, <span class="dt">I =</span> <span class="dv">2</span>, <span class="dt">T1 =</span> <span class="dv">3</span>, <span class="dt">T2 =</span> <span class="dv">0</span>, <span class="dt">L =</span> <span class="dv">0</span>, <span class="dt">Inc =</span> <span class="dv">0</span>)
log.like.theta.bar &lt;-<span class="st"> </span>-<span class="fl">117.5994</span>  <span class="co"># my_particleFilter(SEIT2L_sto,theta.bar, init.state, data=FluTdC1971, n.particles=400)</span>
<span class="kw">print</span>(log.like.theta.bar)
## [1] -117.6

<span class="co"># and its deviance</span>
D.theta.bar &lt;-<span class="st"> </span>-<span class="dv">2</span> *<span class="st"> </span>log.like.theta.bar
<span class="kw">print</span>(D.theta.bar)
## [1] 235.2

<span class="co"># the effective number of parameters</span>
p.D &lt;-<span class="st"> </span><span class="kw">var</span>(-<span class="dv">2</span> *<span class="st"> </span>trace.combined$log.likelihood)/<span class="dv">2</span>
<span class="kw">print</span>(p.D)
## [1] 4.972

<span class="co"># and finally the DIC</span>
DIC &lt;-<span class="st"> </span>D.theta.bar +<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span>p.D
<span class="kw">print</span>(DIC)
## [1] 245.1</code></pre>
<p>In the previous session, we found that the DIC of the deterministic SEIT2L model was equal to 280. The difference of 35 indicates that the stochastic model should strongly be preferred to the deterministic model.</p>
<p>We can visually check this result by plooting the posterior fit of each model:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># take the mean posterior estimates of the deterministic model</span>
x &lt;-<span class="st"> </span><span class="kw">summary</span>(trace.deter)
theta.bar.deter &lt;-<span class="st"> </span>x$statistics[SEIT2L_deter$theta.names, <span class="st">&quot;Mean&quot;</span>]</code></pre>
<pre><code>## Error: object &#39;SEIT2L_deter&#39; not found</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotFit</span>(SEIT2L_sto, theta.bar, init.state, <span class="dt">data =</span> FluTdC1971, <span class="dt">n =</span> <span class="dv">1000</span>)</code></pre>
<pre><code>## Error: object &#39;SEIT2L_sto&#39; not found</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotFit</span>(SEIT2L_deter, theta.bar.deter, init.state, <span class="dt">data =</span> FluTdC1971, <span class="dt">n =</span> <span class="dv">1000</span>)</code></pre>
<pre><code>## Error: object &#39;SEIT2L_deter&#39; not found</code></pre>
<p>Despite the fact that the deterministic model seems to better capture the first peak of the epidemic, the stochastic model better explains the variability of the observed time-series. In particular, the 95% CI of the stochastic model captures almost all the observed data points, even during the first peak.</p>
<p>Already finished? why not <a href="pmcmc.html#going-further">going further?</a></p>
</body>
</html>
