% Solution: the particle MCMC

```{r setup, echo=FALSE}
opts_chunk$set(cache=TRUE, cache.path="cache/pmcmc/", fig.retina=1 ,fig.path="figure/pmcmc/", fig.cap='', fig.height=6, fig.width=8, dpi=100, fig.align="center", message=FALSE, tidy=TRUE, warning=FALSE)
```

```{r fitR, echo=FALSE, results="hide"}
library(fitR)
set.seed(1234)
```

# Run a particle filter

The particle filter returns a Monte-Carlo estimate of the log-likelihood and the precision of this estimate is proportional to the number of particles.

If you have too few particles then you will have a highly variable estimate of the log-likelihood and this will make the exploration of the likelihood surface quite imprecise. In addition, you might experience particle depletion (if you don't know what that means just try to run a particle filter with a single particle).

If you have too many particles, then you will have an accurate estimate of your log-likelihood but it will be very time consuming so inefficient in practice.

However, the precision of the estimate might also depend on the region of the parameter space.

# Calibrate the number of particles

Ideally we want just enough particles to have a fairly stable estimate of your log-likelihood and a reasonable computational time.

A simple calibration approach consists in plotting the mean, standard deviation and computational time of the log-likelihood estimate as a function of the number of particles. Although several parameters can be tested, we can also use a `theta` close to the mean posterior estimate of the deterministic fit since it is likely to be the region of the parameter space we want to explore with the pMCMC.

```{r calibration-SMC, eval=FALSE}
# load fitmodel, data and define init.state
example(SEIT2L_stochastic)
data(FluTdC1971)
init.state <- c("S"=279,"E"=0,"I"=2,"T1"=3,"T2"=0,"L"=0,"Inc"=0) 

# pick a theta close to the mean posterior estimate of the deterministic fit
theta <- c("R0"=7, "D.lat"=1 , "D.inf"=4, "alpha"=0.5, "D.imm"=10, "rho"=0.65)

# vector of number of particles to test
test.n.particles <- seq(50,1000,50)

# number of replicates 
n.replicates <- 100

# vector and data frame of results
sample.log.like <- vector("numeric",length=n.replicates)
res <- data.frame()

for(n.particles in test.n.particles){

	# start measuring time
	start.time  <- Sys.time()
	for(i in 1:n.replicates){
		# one Monte-Carlo estimate of the log-likelihood
		sample.log.like[i] <- my_particleFilter(SEIT2L_sto,theta,init.state,FluTdC1971,n.particles)
	}
	# end measuring time
	end.time  <- Sys.time()

	# keep only replicate with finite log-likelihood to be able to compute the mean and sd
	# this give us the proportion of replicates with particle depletion.
	sample.finite.log.like <- sample.log.like[is.finite(sample.log.like)]

	ans <- c(mean=mean(sample.finite.log.like), sd=sd(sample.finite.log.like), prop.depletion=1-length(sample.finite.log.like)/length(sample.log.like), time=end.time - start.time)

	res <- rbind(res,t(ans))
}
```

We ran this calibration algorithm and obtained the following results:

```{r plot-calibration, echo=FALSE}

data(calibrateSMC)

df_plot <- melt(calibrateSMC,id.vars="n_particles")

df_plot <- mutate(df_plot, variable=revalue(variable,c(time_10000iter_day="time 10000 iter (in days)", prop_depleted="prop. of samples with particle depletion")))

p <- ggplot(df_plot, aes(x=n_particles,y=value))+facet_wrap(~variable,scales="free_y")
p <- p+geom_line()+geom_vline(xintercept=408,col="red")
p <- p+theme_bw()+xlab("number of particles")
print(p)
```

The mean and standard error of the log-likelihood estimates seems to stabilize above 400 particles (red line). By contrast, the computational time increases linearly with the number of particles. Thus, using 400 particles seems to provide a good trade-off between accuracy and computational time. Note however that it will take around 6 days to run 10000 times the particle filter with 400 particles. Since every step of the pMCMC requires to run a particle filter, it becomes clear that some optimization are required to be able to run long chains. We will come back to this discussion later.


# Setting the pMCMC

```{r set-mcmc, eval=FALSE}
# the fitmodel
example(SEIT2L_stochastic)

# wrapper for posterior
my_posteriorSto <- function(theta){

	my_fitmodel <- SEIT2L_sto
	my_init.state <- c(S = 279, E = 0, I = 2, T1 = 3, T2 = 0, L = 0, Inc = 0) 
	my_n.particles <- 400 # you can reduce this value if iterations are too slow but you will loose accuracy in your estimate of the log-likelihood

	return(logPosterior(fitmodel= my_fitmodel, theta=theta, init.state= my_init.state, data=FluTdC1971, margLogLike = my_particleFilter, n.particles=my_n.particles))

}

# load results of deterministic fit
data(mcmc_TdC_deter_longRun)

# Let's use the first trace only, no need to burn or thin
trace <- mcmc_SEITL_infoPrior_theta1$trace

theta <- colMeans(trace[SEIT2L_sto$theta.names])

covmat <- mcmc_SEITL_infoPrior_theta1$covmat.empirical

# lower and upper limits of each parameter
lower <- c(R0=0, D.lat=0 , D.inf=0, alpha=0, D.imm=0, rho=0)
upper <- c(R0=Inf, D.lat=Inf , D.inf=Inf, alpha=1, D.imm=Inf, rho=1)

# number of iterations for the MCMC
n.iterations <- 50 # just a few since it takes quite a lot of time

# Here we don't adapt so that we can check the acceptance rate of the empirical covariance matrix
adapt.size.start <- 100
adapt.size.cooling <- 0.99
adapt.shape.start <- 100

```

You can now go back to the [practical](pmcmc.md#run-a-pmcmc) and try to run a pMCMC with those settings.

# Analyse a pMCMC with 50 particles

Here is an example of analysis of our 5 chains of 3000 iterations with 50 particles.

```{r analyse-n50, collapse=TRUE}
# load traces
data(pmcmc_SEIT2L_infoPrior_n50)

# combine into a `mcmc.list` object
trace <- mcmc.list(lapply(pmcmc_SEIT2L_infoPrior_n50, function(chain) {mcmc(chain$trace)}))

# acceptance rate is quite low
1-rejectionRate(trace)

# accordingly, the combined ESS is quite low
effectiveSize(trace)

# Let's have a look at the traces
xyplot(trace)
```

The burning period looks relatively short, this is because we started the chains with a "good" `init.theta`. That said, since we don't have a relatively short runs it might be useful to carefully choose the burning period with `plotESSBurn`. Note however that this function doesn't accept `mcmc.list` objects.

```{r analyse-n50-burn, collapse=TRUE}
# assuming that the first chain is representative
plotESSBurn(trace[[1]])

# let's burn the 500 first iterations
trace.burn <- burnAndThin(trace, burn=500)

# what about autocorrelation
acfplot(x=trace.burn, lag.max=50)

# ALthough there is significant autocorrelation, we can't too much since the chain are quite short.
# So let's keep 1 iteration every 20
trace.burn.thin.n50 <- burnAndThin(trace.burn, thin=20)

# And finally let's plot the posterior density
densityplot(x=trace.burn.thin.n50)
```

The 5 chains shows quite different posterior distributions, with different shapes and modes. In addition, the ESS is too small to have smooth posterior distributions.

With so few particles, the likelihood estimate is very noisy and the exploration of the parameter space is not efficient. Furthermore, since the accuracy of the posterior rely on the accuracy of the likelihood, the posterior estimates might be biased. __Not sure: since the estimate of the likelihood is unbiased I wonder whether the posterior would be accurate if we run a very long chain?__

You can now return to the [practical](pmcmc.md#analyse-a-pmcmc-with-400-particles) and analyse a pMCMC with much more particles that we ran for you.


# Analyse a pMCMC with 400 particles

Here is an example of analysis of our 5 chains of 3000 iterations with 400 particles.

```{r analyse-n400, collapse=TRUE}
# load traces
data(pmcmc_SEIT2L_infoPrior_n400)

# combine into a `mcmc.list` object
trace <- mcmc.list(lapply(pmcmc_SEIT2L_infoPrior_n400, function(chain) {mcmc(chain$trace)}))

# acceptance rate is optimal
1-rejectionRate(trace)

# Note that the combined ESS is 5 times higher than with 50 particles
effectiveSize(trace)

# Let's have a look at the traces
xyplot(trace)
```

As in the analysis with 50 particles, the burning period is relatively short. However, with 400 particles the chains mixe much better. Of course the chains take longer to run but because they converge quickly it becomes computationally more efficient to run many short chains in parallel, starting near the mode of the (deterministic) posterior, and combine them to increase the ESS.

Here again we decide to carefully choose the burning period with `plotESSBurn`.

```{r analyse-n400-burn, collapse=TRUE}
plotESSBurn(trace[[1]])

# let's burn the 250 first iterations
trace.burn <- burnAndThin(trace, burn=250)

# what about autocorrelation?
acfplot(x=trace.burn, lag.max=50)

# Autocorrelation decreases much more quickly than with 50 particles
# Let's keep 1 iteration every 20
trace.burn.thin.n400 <- burnAndThin(trace.burn, thin=20)

# Let's plot the posterior densities
densityplot(x=trace.burn.thin.n400)
```

All 5 chains seems to have converged to the same posterior, which are smoother than with 50 particles. Let's compare the combined posterior densities with that obtained with 50 particles

```{r n400-n50}
plotPosteriorDensity(list(n50=trace.burn.thin.n50,n400=trace.burn.thin.n400))
```
Overall, the posterior distributions differ substantially. In particular, the location of the mode of $R_0$ is shifted to the left when there are too few particles. That said, some posterior are relatively similar ($D_{lat}$, $D_{inf}$).

Finally, note that the log-likelihood is overestimated with n50, which can be problematic when doing model selection as we would overestimate the fit of the model.

You can now return to the [practical](pmcmc.md#stochastic-vs-deterministic-fit) and proceed to the last section of this session.

# Stochastic vs deterministic fit

Here we compare the combined traces of the deterministic SEIT2L model (2 chains of $10^5$ iterations) with those obtained with the stochastic version (5 chains of $3000$ iterations). Both analysis assume informative priors.

```{r sto-deter, collapse=TRUE}

# load, burn and thin the deterministic fit

# create mcmc object
trace1 <- mcmc(mcmc_SEIT2L_infoPrior_theta1$trace)
trace2 <- mcmc(mcmc_SEIT2L_infoPrior_theta2$trace)

# combine in a mcmc.list
trace <- mcmc.list(trace1,trace2)

# burn and thin as the chain with uniform prior (see above sections)
trace.deter <-  burnAndThin(trace, burn=5000, thin=40)

# compare posterior density
plotPosteriorDensity(list(deter=trace.deter, sto=trace.burn.thin.n400))
```

Overall, the posterior distributions are quite different. This is especially true for $R_0$ and $D_{imm}$.
In addition, the discrepancy in the posterior distribution of the log-likelihood seems to indicate that the stochastic model fits much better.
We can verify this by computing the DIC of the stochastic SEIT2L model.

```{r sto-deter-DIC, collapse=TRUE}
# combine all traces in a data frame
trace.combined <- ldply(trace.burn.thin.n400)

# take the mean of theta
theta.bar <- colMeans(trace.combined[SEIT2L_sto$theta.names])
print(theta.bar)

# compute its log-likelihood
init.state <- c(S = 279, E = 0, I = 2, T1 = 3, T2 =0, L = 0, Inc = 0)
log.like.theta.bar <- -117.5994 # my_particleFilter(SEIT2L_sto,theta.bar, init.state, data=FluTdC1971, n.particles=400)
print(log.like.theta.bar)

# and its deviance
D.theta.bar <- -2*log.like.theta.bar
print(D.theta.bar)

# the effective number of parameters
p.D <- var(-2*trace.combined$log.likelihood)/2
print(p.D)

# and finally the DIC
DIC <- D.theta.bar + 2*p.D
print(DIC)
```

In the previous session, we found that the DIC of the deterministic SEIT2L model was equal to 280. The difference of 35 indicates that the stochastic model should strongly be preferred to the deterministic model.

We can visually check this result by plooting the posterior fit of each model:

```{r sto-deter-fit, results="hide"}

# take the mean posterior estimates of the deterministic model
x <- summary(trace.deter)
theta.bar.deter <- x$statistics[SEIT2L_deter$theta.names,"Mean"]

plotFit(SEIT2L_sto,theta.bar,init.state,data=FluTdC1971,n=1000)

plotFit(SEIT2L_deter,theta.bar.deter,init.state,data=FluTdC1971,n=1000)

```

Despite the fact that the deterministic model seems to better capture the first peak of the epidemic, the stochastic model better explains the variability of the observed time-series. In particular, the 95% CI of the stochastic model captures almost all the observed data points, even during the first peak.


Already finished? why not [going further?](pmcmc.md#going-further)


















