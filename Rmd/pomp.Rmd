---
title: "Fitting models using *pomp*"
--- 

```{r setup, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE, fig.path='figure/pmcmc/', cache.path='cache/pmcmc/' , fig.cap='', fig.align="center", message=FALSE, tidy=TRUE, warning=FALSE)
```

```{r fitR, echo=FALSE, cache=FALSE, results="hide"}
library(fitR)
set.seed(1234)
```
[Lecture slides](slides/pomp_slides.pdf)

# Objectives

The aim of this session is to introduce the *R* package *pomp*, which contains functions for many of the tasks we have performed in this course. The benefit of using a package like *[pomp](http://pomp.r-forge.r-project.org/)* (or alternatives such as [SSM](https://github.com/sballesteros/ssm) or [libbi](http://libbi.org/)) is that they have been optimised for computational efficiency and can take full advantage of any available hardware (including running on a high-performance cluster). The disadvantage of using readily available packages is that 1) to allow for good performance, models are usually not coded in *R* and 2) it can be more difficult to debug when something is going wrong.

The aim of this session is to see how the different methods we have encountered in the course can be applied to fit a model to data using *pomp*. 

In the [previous session](pmcmc.html), you have coded a particle filter to estimate the likelihood, and the function `mcmcMH` to sample from the posterior distribution with a Metropolis-Hastings algorithm.

In this session you will:

1. learn how to code a model in pomp using C snippets
2. explore how to use a particle filter and pMCMC using pomp
3. explore a method for real-time modelling

# Code a model in pomp

To do model fitting with pomp, you need to create a pomp object. This works a bit like the fitmodel objects we created earlier. To create a pomp object, you need to specify a data set as well as functions that simulates the model, evaluates the prior density, etc. Which components and functions exactly are needed to construct a pomp object depends on the method that one wants to use. For more information on this, you can have a look at the recent [article](http://pomp.r-forge.r-project.org/vignettes/pompjss.pdf) on pomp in the Journal of Statistical Software.

To load the pomp library, type
```{r}
library('pomp')
```

If this does not work, you need to install the pomp library. You can do this using
```{r eval = FALSE}
install.packages('pomp')
```

To specify a model that you want to fit to data in pomp, you can either write a function in R, or you can use so-called C snippets, that is model code written in C that is pre-compiled and can be called from an R function. The advantage of doing this is that you can benefit from the speed of compiled C code while not having to learn an awful lot about the details of C syntax.

We have coded up the models of the previous practical sessions in pomp for you. To look at, for example, the SEITL model, use

```{r}
example(SEITL_pomp)
```

(if this yields a compile error, have a look at the [important information for windows and mac users](http://pomp.r-forge.r-project.org/vignettes/getting_started.html#important-information-for-windows-and-mac-users) or ask us for help).

*Take 10 minutes* to have a look at the code that is printed when you execute this command, and try to understand what it does. You can have a look at our [more detailed explanation](pomp_seitl_explanation.html) for further help.

The pomp object now SEITL_pomp now contains everything needed for model fitting and inference. 

```{r skeleton-PF, eval=FALSE, tidy=FALSE}
# This is a function that takes four parameters:
# - fitmodel: a fitmodel object
# - theta: named numeric vector. Values of the parameters for which the marginal log-likelihood is desired.
# - init.state: named numeric vector. Initial values of the state variables.
# - data: data frame. Observation times and observed data.
# The function returns the value of the marginal log-likelihood
my_particleFilter <- function(fitmodel, theta, init.state, data, n.particles) {

    ## Initialisation of the algorithm
    # Initialise the state and the weight of your particles

    ## Start for() loop over observation times

        # Resample particles according to their weights
        # You can use the `sample() function of R

        ## Start for() loop over particles

            # Propagate the particle from current observation time to the next one
            # using the function `fitmodel$simulate`

            # Weight the particle with the likelihood of the observed data point
            # using the function `fitmodel$dPointObs`

        ## End for() loop over particles

    ## End for() loop over observation times

    ## Compute and return the marginal log-likelihood
    # sum of the log of the mean of the weights at each observation time

}
```

```{r load-PF, echo=FALSE}
source("our_smc.r")
```

If you have trouble filling any of the empty bits, have a look at our [more guided example](smc_example.html).

# Run a particle filter

Try to run your particle filter with the following inputs:

```{r try-PF, collapse=TRUE}
# load SEIT2L_stoch
data(SEIT2L_stoch)

# load data
data(FluTdC1971)

# theta close to the mean posterior estimate of the deterministic SEIT2L model
theta <- c("R0" = 7, "D_lat" = 1 , "D_inf" = 4, "alpha" = 0.5, "D_imm" = 10, "rho" = 0.65)

# init state as before
init.state <- c("S" = 279,"E" = 0,"I" = 2,"T1" = 3,"T2" = 0,"L" = 0,"Inc" = 0) 

# run the particle filter with 20 particles 
my_particleFilter(SEIT2L_stoch, theta, init.state,  data = FluTdC1971,  n.particles = 20)
```

Does your particle filter return the same value for the marginal log-likelihood? Can you explain why?

What can you notice when you:

* Run several replicates with the same number of particles  
* Increase the number of particles
* Decrease the number of particles (try with one particle)
* Change `theta`

# Calibrate the number of particles

Can you think of and implement an algorithm to calibrate the number of particles? 

Compare your approach with [ours](pmcmc_solution.html#calibrate-the-number-of-particles) and determine an optimal number of particles. 

# Run pMCMC

You can now write a new wrapper for the function `logPosterior` that will take `margLogLike = my_particleFilter` as argument. This wrapper will then be passed to `mcmcMH`.

Note that the function `logPosterior` doesn't have a `n.particles` argument so you might wonder how to specify it to `my_particleFilter`? Have a look at the documentation of `logPosterior`. You should notice the `...` argument, which allows you to pass any extra argument to the function `margLogLike`.

You have probably noticed that running `my_particleFilter` is time consuming. As we mentioned in the [previous session](mcmc_and_model_comparison.html#objectives) this can lead to a waste of time and computational resources if you initialise the pMCMC with parameter values far from the target and with a Gaussian proposal that is very different from the target. 

In order to efficiently initialise your pMCMC, you can make use the results of the fit of the deterministic SEIT2L model of the previous session. Remember that you can load these results as follows:

```{r load-SEIT2L, collapse=TRUE}
data(mcmc_TdC_deter_longRun)
# this should load 2 objects in your environment: mcmc_SEIT2L_infoPrior_theta1 and mcmc_SEIT2L_infoPrior_theta2. Each one is a list of 3 elements returned by mcmcMH
names(mcmc_SEIT2L_infoPrior_theta1)
```

In the previous session, you have initialised `mcmcMH` with a diagonal covariance matrix for the Gaussian proposal using the argument `proposal.sd`. Actually, you can also pass a non-diagonal covariance matrix, accounting for correlations, by using the argument `covmat` (type `?mcmcMH` for more details).

You can now set the pMCMC by filling the empty bits below:

```{r set-pmcmc, eval=FALSE, tidy=FALSE}

# wrapper for posterior
my_posteriorSto <- function(theta){

    my_fitmodel <- # INSERT HERE
    my_init.state <- # INSERT HERE
    my_n.particles <- # INSERT HERE

    return(logPosterior(fitmodel = my_fitmodel,
                        theta = theta,
                        init.state = my_init.state, 
                        data = FluTdC1971,
                        margLogLike = my_particleFilter,
                        n.particles = my_n.particles))
    
}

# theta to initialise the pMCMC
init.theta <- # INSERT HERE

# covariance matrix for the Gaussian proposal
covmat <- # INSERT HERE

# lower and upper limits of each parameter (must be named vectors)
lower <- # INSERT HERE
upper <- # INSERT HERE

# number of iterations for the pMCMC
n.iterations <- # INSERT HERE

# additional parameters for the adaptive pMCMC, see ?mcmcMH for more details
adapt.size.start <- # INSERT HERE
adapt.size.cooling <- # INSERT HERE
adapt.shape.start <- # INSERT HERE
```

If you have trouble filling some of the empty bits, have a look at our [solution](pmcmc_solution.html#setting-the-pmcmc).

Then you should be able to run `mcmcMH`:

```{r run-pmcmc, eval=FALSE, tidy = FALSE}
# run the pMCMC
my_pMCMC <- mcmcMH(target = my_posteriorSto,
                   init.theta = init.theta,
                   covmat = covmat,
                   limits = list(lower = lower,upper = upper),
                   n.iterations = n.iterations,
                   adapt.size.start = adapt.size.start,
                   adapt.size.cooling = adapt.size.cooling,
                   adapt.shape.start = adapt.shape.start)
```

# Analyse a pMCMC with 50 particles

If you have run a pMCMC you should have noticed that it takes quite a lot of time as we need to run a particle filter at each iteration. Since the computation time scales linearly with the number of particles you might be tempted to reduce this number. Let's have a look at what we would get by running a pMCMC with 50 particles.

To save time, we have run 5 chains in parallel for you. Each chain was started from a different `init.theta` and ran for 3000 iterations. The `init.theta` were chosen close to the mean posterior estimates of the deterministic fit and it's empirical covariance matrix was also used for the Gaussian proposal kernel. Each chain took 6 hours to complete on a scientific computing cluster and can be loaded as follows:

```{r load-trace, collapse=TRUE}
data(pmcmc_SEIT2L_infoPrior_n50)
# this should load a list with the same name, which contains the 5 chains.
names(pmcmc_SEIT2L_infoPrior_n50)
# each chain is a list of 3 elements returned by mcmcMH
names(pmcmc_SEIT2L_infoPrior_n50[["chain1"]])
# the trace contains 9 variables for 3000 iterations
dim(pmcmc_SEIT2L_infoPrior_n50[["chain1"]]$trace)
``` 
We can combine the traces of the 5 chains into a `mcmc.list` object as follows:

```{r combine-trace, collapse=TRUE}
trace <- mcmc.list(lapply(pmcmc_SEIT2L_infoPrior_n50, function(chain) {mcmc(chain$trace)}))
head(trace,1)
```
You can check that the chains were started from different `init.state` and that we used informative priors.

__Take 15 min__ to analyse these chains and conclude on the choice of using 50 particles to save computation time.

You can compare your conclusions with [ours](pmcmc_solution.html#analyse-a-pmcmc-with-50-particles).

# Analyse a pMCMC with 400 particles

Let's increase the number of particles to 400, which is suggested by the calibration analysis. Again, to save time we have run 5 chains of 3000 iterations in parallel. Each chain took 40 hours to complete on a scientific computing cluster and can be loaded and stored in a `mcmc.list` object as follows:

```{r load-trace-400, collapse=TRUE}
# load
data(pmcmc_SEIT2L_infoPrior_n400)
# create 
trace <- mcmc.list(lapply(pmcmc_SEIT2L_infoPrior_n50, function(chain) {mcmc(chain$trace)}))
``` 

Re-do the same analysis as for 50 particles. What differences can you notice? In particular, try to compare the posterior distributions with 50 and 400 particles using the function `plotPosteriorDensity`.

You can also have a look at our [solution](pmcmc_solution.html#analyse-a-pmcmc-with-400-particles) if that helps.

# Stochastic vs deterministic fit

Compare the posterior of the deterministic and stochastic SEIT2L models. Which model provides the best fit? Can you explain why?

Check your answer [here](pmcmc_solution.html#stochastic-vs-deterministic-fit).

# Going further

## Filtered trajectories

* Actually, in addition to the log-likelihood, a particle filter can also return the filtered trajectories (i.e. all the trajectories that "survived" until the last observation time). You can update your filter so it keeps track and returns the filtered trajectories. Alternatively, there is a function in the package called `particleFilter` that will do it for you (see `?particleFilter` for documentation). Have a look at its code.
* If you run a particle filter with our function `particleFilter`, you can then plot the filtered trajectories using the function `plotSMC`.

## Optimization

You might have noted that the `for()` loop over particles could be parallelized, as particles can be propagated independently. You could take advantage of this to code a parallel loop and make your algorithm even faster. If you have never coded a parallel program in R you can also have a look at the code of `particleFilter`. Actually, all the test runs were performed on a scientific computing cluster with 12 core machines. So the computational time is expected to be multiplied by 12 without parallelization of the particle filter.

<div>
# Navigate
Top: [Index](index.html) Previous: [MCMC and model comparison](mcmc_and_model_comparison.html) Next: [Approximate Bayesian Computation](abc.html)
</div>
